import logging
import json
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from uuid import UUID
import uuid
from app.config import settings
from app.services.kb_retrieval import KnowledgeBaseRetriever
from app.services.core_client import CoreAPIClient

logger = logging.getLogger(__name__)

# â”€â”€ å¯é¸ä¾è³´ â”€â”€
try:
    import openai as openai_lib
    _HAS_OPENAI = True
except ImportError:
    _HAS_OPENAI = False


class ChatOrchestrator:
    """
    èŠå¤©å”èª¿å™¨ï¼ˆRAG Generation å±¤ï¼‰

    è² è²¬ï¼š
    1. ä¸¦è¡ŒæŸ¥è©¢å…¬å¸å…§è¦ + å‹è³‡æ³• Core API
    2. ä½¿ç”¨ LLM æ ¹æ“šæª¢ç´¢çµæœç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”
    3. é™„å¸¶ä¾†æºå¼•ç”¨èˆ‡æ³•å¾‹å…è²¬è²æ˜
    4. æ”¯æ´ä¸²æµç”Ÿæˆ (T7-1) èˆ‡å¤šè¼ªå°è©± (T7-2)
    """

    SYSTEM_PROMPT = """ä½ æ˜¯ UniHR äººè³‡ AI åŠ©ç†ï¼Œå°ˆé–€å›ç­”å°ç£ä¼æ¥­çš„äººäº‹è¦ç« èˆ‡å‹å‹•æ³•è¦å•é¡Œã€‚

å›ç­”è¦å‰‡ï¼š
1. **åªæ ¹æ“šä¸‹æ–¹æä¾›çš„åƒè€ƒè³‡æ–™å›ç­”**ï¼Œä¸è¦è‡ªè¡Œæé€ æˆ–å¼•ç”¨æœªæä¾›çš„å…§å®¹
2. å¦‚æœæœ‰å…¬å¸å…§è¦ï¼Œä»¥å…¬å¸å…§è¦ç‚ºä¸»ï¼Œæ³•å¾‹è¦å®šç‚ºè¼”åŠ©åƒç…§
3. å¦‚æœå…¬å¸å…§è¦çš„è¦å®š**ä½æ–¼**å‹å‹•æ³•çš„æœ€ä½æ¨™æº–ï¼Œå¿…é ˆæ˜ç¢ºæŒ‡å‡º
4. ä½¿ç”¨çµæ§‹åŒ–æ ¼å¼ï¼ˆæ¨™é¡Œã€æ¢åˆ—ï¼‰è®“å›ç­”æ¸…æ¥šæ˜“è®€
5. å¼•ç”¨è³‡æ–™æ™‚æ¨™è¨»ä¾†æºï¼ˆæª”åæˆ–æ³•æ¢åç¨±ï¼‰
6. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œå¦ç™½èªªæ˜ä¸¦å»ºè­°è«®è©¢ HR éƒ¨é–€
7. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”"""

    FOLLOWUP_PROMPT = """

åœ¨å›ç­”çš„æœ€å¾Œï¼Œè«‹å¦èµ·ä¸€è¡Œè¼¸å‡º 2-3 å€‹ä½¿ç”¨è€…å¯èƒ½æœƒè¿½å•çš„å»ºè­°å•é¡Œï¼Œæ ¼å¼ï¼š
[å»ºè­°å•é¡Œ]
1. ...
2. ...
3. ..."""
    
    def __init__(self):
        self.kb_retriever = KnowledgeBaseRetriever()
        self.core_client = CoreAPIClient()

        # OpenAI client (sync + async)
        self._openai = None
        self._openai_async = None
        openai_key = getattr(settings, "OPENAI_API_KEY", "")
        if _HAS_OPENAI and openai_key:
            self._openai = openai_lib.OpenAI(api_key=openai_key)
            self._openai_async = openai_lib.AsyncOpenAI(api_key=openai_key)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T7-0: æª¢ç´¢å±¤ï¼ˆèˆ‡ç”Ÿæˆè§£è€¦ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def retrieve_context(
        self,
        tenant_id: UUID,
        question: str,
        top_k: int = 3,
    ) -> Dict[str, Any]:
        """
        ç´”æª¢ç´¢ï¼šä¸¦è¡ŒæŸ¥è©¢å…¬å¸å…§è¦ + å‹è³‡æ³• Core APIï¼Œå›å‚³çµæ§‹åŒ–ä¸Šä¸‹æ–‡ã€‚
        
        åˆ†é›¢è‡ªåŸ process_queryï¼Œä½¿ä¸²æµç«¯é»å¯å…ˆå–å¾—ä¾†æºï¼Œå†åˆ†æ®µç”Ÿæˆã€‚
        """
        request_id = str(uuid.uuid4())

        async def get_company_policy():
            try:
                results = self.kb_retriever.search(
                    tenant_id=tenant_id,
                    query=question,
                    top_k=top_k,
                )
                return {"status": "success", "results": results}
            except Exception as e:
                return {"status": "error", "error": str(e), "results": []}

        async def get_labor_law():
            try:
                result = await self.core_client.chat(
                    question=question,
                    request_id=request_id,
                )
                return result
            except Exception as e:
                return {"status": "error", "answer": "å‹è³‡æ³•æŸ¥è©¢å¤±æ•—", "error": str(e)}

        company_policy_result, labor_law_result = await asyncio.gather(
            asyncio.create_task(get_company_policy()),
            asyncio.create_task(get_labor_law()),
        )

        # â”€â”€ çµ„è£çµæ§‹åŒ–ä¸Šä¸‹æ–‡ â”€â”€
        return self._build_context(
            question=question,
            company_policy=company_policy_result,
            labor_law=labor_law_result,
            request_id=request_id,
        )

    def _build_context(
        self,
        question: str,
        company_policy: Dict[str, Any],
        labor_law: Dict[str, Any],
        request_id: str,
    ) -> Dict[str, Any]:
        """å°‡ raw æª¢ç´¢çµæœçµ„è£ç‚ºçµæ§‹åŒ– context dictã€‚"""
        has_policy = (
            company_policy.get("status") == "success"
            and len(company_policy.get("results", [])) > 0
        )
        has_labor_law = (
            labor_law.get("status") != "error" and labor_law.get("answer")
        )

        context: Dict[str, Any] = {
            "request_id": request_id,
            "question": question,
            "has_policy": has_policy,
            "has_labor_law": has_labor_law,
            "company_policy_raw": None,
            "labor_law_raw": None,
            "context_parts": [],
            "sources": [],
            "disclaimer": "æœ¬å›ç­”åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ­£å¼æ³•å¾‹æ„è¦‹ã€‚å¦‚æœ‰å…·é«”æƒ…æ³ï¼Œè«‹è«®è©¢å°ˆæ¥­æ³•å¾‹é¡§å•ã€‚",
        }

        if has_policy:
            top_policies = company_policy["results"][:3]
            context["company_policy_raw"] = {
                "content": top_policies[0]["content"],
                "source": top_policies[0]["filename"],
                "relevance_score": top_policies[0]["score"],
                "all_results": [
                    {
                        "content": r["content"][:500],
                        "filename": r["filename"],
                        "score": r["score"],
                    }
                    for r in top_policies
                ],
            }
            for r in top_policies:
                context["sources"].append({
                    "type": "policy",
                    "title": r["filename"],
                    "snippet": r["content"][:200],
                    "score": r["score"],
                })
            for i, r in enumerate(top_policies, 1):
                context["context_parts"].append(
                    f"ã€å…¬å¸å…§è¦ #{i}ã€‘ï¼ˆä¾†æºï¼š{r['filename']}ï¼Œç›¸é—œåº¦ï¼š{r['score']:.2f}ï¼‰\n{r['content']}"
                )

        if has_labor_law:
            context["labor_law_raw"] = {
                "answer": labor_law.get("answer", ""),
                "citations": labor_law.get("citations", []),
                "usage": labor_law.get("usage", {}),
            }
            if labor_law.get("citations"):
                for citation in labor_law["citations"]:
                    law_name = citation.get("law_name") or "å‹å‹•æ³•è¦"
                    article = citation.get("article") or ""
                    title = f"{law_name} {article}".strip()
                    context["sources"].append({
                        "type": "law",
                        "title": title,
                        "snippet": labor_law.get("answer", "")[:200],
                    })
            law_text = labor_law.get("answer", "")
            citations_text = ""
            if labor_law.get("citations"):
                citations_text = "ï¼›".join(
                    f"{c.get('law_name', '')} {c.get('article', '')}"
                    for c in labor_law["citations"]
                )
            context["context_parts"].append(
                f"ã€å‹å‹•æ³•è¦ã€‘{citations_text}\n{law_text}"
            )

        return context

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T7-1: ä¸²æµç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def stream_answer(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
        include_followup: bool = True,
    ) -> AsyncGenerator[str, None]:
        """
        ä¸²æµç”Ÿæˆ LLM å›ç­”ï¼ˆSSE ç”¨ï¼‰ã€‚

        yield æ¯å€‹ token chunkï¼Œå‰ç«¯å¯é€å­—æ¸²æŸ“ã€‚
        è‹¥ LLM ä¸å¯ç”¨ï¼Œå‰‡ yield æ•´æ®µ fallbackã€‚
        """
        if not self._openai_async or not (context["has_policy"] or context["has_labor_law"]):
            yield self._fallback_answer(context)
            return

        messages = self._build_llm_messages(
            question, context, history=history, include_followup=include_followup
        )

        try:
            response = await self._openai_async.chat.completions.create(
                model=getattr(settings, "OPENAI_MODEL", "gpt-4o-mini"),
                messages=messages,
                temperature=getattr(settings, "OPENAI_TEMPERATURE", 0.3),
                max_tokens=getattr(settings, "OPENAI_MAX_TOKENS", 1500),
                stream=True,
            )
            async for chunk in response:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content
        except Exception as e:
            logger.warning(f"LLM ä¸²æµç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°æ¨¡æ¿: {e}")
            yield self._fallback_answer(context)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T7-2: å¤šè¼ªå°è©±æ”¯æ´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def contextualize_query(
        self, query: str, history: List[Dict[str, str]]
    ) -> str:
        """
        ç”¨ LLM å°‡å«ä»£åè©/çœç•¥ä¸»è©çš„æŸ¥è©¢æ”¹å¯«ç‚ºç¨ç«‹æŸ¥è©¢ã€‚
        è‹¥æ­·å²ç‚ºç©ºæˆ– LLM ä¸å¯ç”¨ï¼Œç›´æ¥å›å‚³åŸ queryã€‚
        """
        if not history or not self._openai_async:
            return query

        messages = [
            {
                "role": "system",
                "content": (
                    "æ ¹æ“šå°è©±æ­·å²ï¼Œå°‡ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œæ”¹å¯«ç‚ºä¸€å€‹ç¨ç«‹ã€å®Œæ•´çš„æŸ¥è©¢ã€‚"
                    "åªè¼¸å‡ºæ”¹å¯«å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦è§£é‡‹ã€‚å¦‚æœå•é¡Œå·²ç¶“å¤ æ˜ç¢ºï¼Œç›´æ¥åŸæ¨£è¼¸å‡ºã€‚"
                ),
            },
            *[{"role": m["role"], "content": m["content"]} for m in history[-4:]],
            {"role": "user", "content": query},
        ]

        try:
            response = await self._openai_async.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0,
                max_tokens=200,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.warning(f"æŸ¥è©¢æ”¹å¯«å¤±æ•—: {e}")
            return query

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å‘ä¸‹ç›¸å®¹ï¼šä¿ç•™åŸ process_query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def process_query(
        self,
        tenant_id: UUID,
        question: str,
        top_k: int = 3,
        conversation_id: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> Dict[str, Any]:
        """
        è™•ç†ç”¨æˆ¶æŸ¥è©¢ï¼ˆéä¸²æµï¼Œå‘ä¸‹ç›¸å®¹ï¼‰ã€‚
        
        æ–°å¢ conversation_id / history åƒæ•¸ä»¥æ”¯æ´å¤šè¼ªå°è©±ã€‚
        """
        # æŸ¥è©¢æ”¹å¯«ï¼ˆå¤šè¼ªï¼‰
        effective_question = question
        if history:
            effective_question = await self.contextualize_query(question, history)

        # æª¢ç´¢
        ctx = await self.retrieve_context(
            tenant_id=tenant_id,
            question=effective_question,
            top_k=top_k,
        )

        # ç”Ÿæˆå›ç­”ï¼ˆéä¸²æµï¼‰
        result = {
            "request_id": ctx["request_id"],
            "question": question,
            "company_policy": ctx["company_policy_raw"],
            "labor_law": ctx["labor_law_raw"],
            "answer": "",
            "sources": ctx["sources"],
            "notes": [],
            "disclaimer": ctx["disclaimer"],
        }

        if self._openai and (ctx["has_policy"] or ctx["has_labor_law"]):
            try:
                result["answer"] = self._generate_answer_sync(
                    question, ctx, history=history
                )
                result["notes"].append("ç”± AI æ ¹æ“šæª¢ç´¢çµæœç”Ÿæˆå›ç­”")
            except Exception as e:
                logger.warning(f"LLM å›ç­”ç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°æ¨¡æ¿: {e}")
                result["answer"] = self._fallback_answer(ctx)
                result["notes"].append("LLM æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œä»¥çµæ§‹åŒ–æ ¼å¼å‘ˆç¾")
        else:
            result["answer"] = self._fallback_answer(ctx)
            if not (ctx["has_policy"] or ctx["has_labor_law"]):
                result["notes"].append("æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š")

        return result

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM Messages çµ„è£ï¼ˆå…±ç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_llm_messages(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
        include_followup: bool = True,
    ) -> List[Dict[str, str]]:
        """çµ„è£ LLM çš„ messages é™£åˆ—ï¼ˆå«æ­·å² + æª¢ç´¢ä¸Šä¸‹æ–‡ï¼‰ã€‚"""
        system_content = self.SYSTEM_PROMPT
        if include_followup:
            system_content += self.FOLLOWUP_PROMPT

        messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_content}
        ]

        # æ³¨å…¥æ­·å²ï¼ˆToken é ç®—ç®¡ç†ï¼‰
        if history:
            max_history_tokens = 2000
            total_tokens = 0
            history_msgs = []
            for msg in reversed(history):
                # ç²—ä¼° 1 ä¸­æ–‡å­— â‰ˆ 2 tokens
                msg_tokens = len(msg["content"])
                if total_tokens + msg_tokens > max_history_tokens:
                    break
                history_msgs.insert(0, {"role": msg["role"], "content": msg["content"]})
                total_tokens += msg_tokens
            messages.extend(history_msgs)

        context_text = "\n\n".join(context["context_parts"])
        user_content = f"å•é¡Œï¼š{question}\n\nåƒè€ƒè³‡æ–™ï¼š\n{context_text}\n\nè«‹æ ¹æ“šä¸Šè¿°åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚"
        messages.append({"role": "user", "content": user_content})

        return messages

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŒæ­¥ç”Ÿæˆï¼ˆç›¸å®¹åŸä»‹é¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _generate_answer_sync(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """åŒæ­¥ LLM ç”Ÿæˆå›ç­”ï¼ˆéä¸²æµï¼‰ã€‚"""
        messages = self._build_llm_messages(question, context, history=history)

        response = self._openai.chat.completions.create(
            model=getattr(settings, "OPENAI_MODEL", "gpt-4o-mini"),
            messages=messages,
            temperature=getattr(settings, "OPENAI_TEMPERATURE", 0.3),
            max_tokens=getattr(settings, "OPENAI_MAX_TOKENS", 1500),
        )
        return response.choices[0].message.content.strip()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _fallback_answer(context: Dict[str, Any]) -> str:
        """LLM ä¸å¯ç”¨æ™‚çš„æ¨¡æ¿ fallbackã€‚"""
        has_policy = context.get("has_policy", False)
        has_labor_law = context.get("has_labor_law", False)

        if has_policy and has_labor_law:
            policy_content = context["company_policy_raw"]["content"][:500]
            law_answer = context["labor_law_raw"]["answer"][:500]
            return f"""ğŸ“‹ **å…¬å¸å…§è¦è¦å®š**ï¼š
{policy_content}

âš–ï¸ **å‹å‹•æ³•è¦è£œå……**ï¼š
{law_answer}

ğŸ’¡ **èªªæ˜**ï¼šå…¬å¸å…§è¦æ˜¯æ‚¨çš„å„ªå…ˆåƒè€ƒä¾æ“šï¼Œä½†ä¸å¾—é•åå‹å‹•æ³•çš„æœ€ä½æ¨™æº–ã€‚"""

        elif has_policy:
            return f"""ğŸ“‹ **å…¬å¸å…§è¦è¦å®š**ï¼š
{context["company_policy_raw"]["content"]}

ğŸ’¡ **æé†’**ï¼šæœªæŸ¥è©¢åˆ°ç›¸é—œå‹å‹•æ³•è¦è£œå……ã€‚å¦‚éœ€äº†è§£æ³•å¾‹æœ€ä½æ¨™æº–ï¼Œè«‹é€²ä¸€æ­¥è«®è©¢ã€‚"""

        elif has_labor_law:
            return f"""âš–ï¸ **å‹å‹•æ³•è¦**ï¼š
{context["labor_law_raw"]["answer"]}

ğŸ’¡ **æé†’**ï¼šæœªåœ¨å…¬å¸å…§è¦ä¸­æ‰¾åˆ°ç›¸é—œè¦å®šã€‚å»ºè­°ç¢ºèªå…¬å¸æ˜¯å¦æœ‰é¡å¤–è¦å®šã€‚"""

        else:
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚è«‹å˜—è©¦æ›å€‹æ–¹å¼æå•ï¼Œæˆ–è¯ç¹« HR éƒ¨é–€ã€‚"

    def format_summary(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ‘˜è¦ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰"""
        summary = f"**å•é¡Œ**ï¼š{result['question']}\n\n"
        summary += result["answer"]

        if result["sources"]:
            summary += "\n\n**åƒè€ƒä¾†æº**ï¼š\n"
            for source in result["sources"]:
                if source["type"] == "company_policy":
                    summary += f"- ğŸ“‹ {source['filename']} (ç›¸é—œåº¦: {source['score']:.2f})\n"
                elif source["type"] == "labor_law":
                    summary += f"- âš–ï¸ {source.get('law_name', 'å‹å‹•æ³•è¦')} {source.get('article', '')}\n"

        summary += f"\n\n{result['disclaimer']}"
        return summary
