import logging
import json
import re
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from uuid import UUID
import uuid
from app.config import settings
from app.services.kb_retrieval import KnowledgeBaseRetriever
from app.services.core_client import CoreAPIClient
from app.services.structured_answers import try_structured_answer

logger = logging.getLogger(__name__)

# â”€â”€ å¯é¸ä¾è³´ â”€â”€
try:
    import openai as openai_lib
    _HAS_OPENAI = True
except ImportError:
    _HAS_OPENAI = False


class ChatOrchestrator:
    """
    èŠå¤©å”èª¿å™¨ï¼ˆRAG Generation å±¤ï¼‰

    è² è²¬ï¼š
    1. ä¸¦è¡ŒæŸ¥è©¢å…¬å¸å…§è¦ + å‹è³‡æ³• Core API
    2. ä½¿ç”¨ LLM æ ¹æ“šæª¢ç´¢çµæœç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”
    3. é™„å¸¶ä¾†æºå¼•ç”¨èˆ‡æ³•å¾‹å…è²¬è²æ˜
    4. æ”¯æ´ä¸²æµç”Ÿæˆ (T7-1) èˆ‡å¤šè¼ªå°è©± (T7-2)
    """

    SYSTEM_PROMPT = """ä½ æ˜¯ UniHR äººè³‡ AI åŠ©ç†ï¼Œå°ˆé–€å›ç­”å°ç£ä¼æ¥­çš„äººäº‹è¦ç« èˆ‡å‹å‹•æ³•è¦å•é¡Œã€‚

å›ç­”è¦å‰‡ï¼š
1. **åªæ ¹æ“šä¸‹æ–¹æä¾›çš„åƒè€ƒè³‡æ–™å›ç­”**ï¼Œä¸è¦è‡ªè¡Œæé€ æˆ–å¼•ç”¨æœªæä¾›çš„å…§å®¹
2. å¦‚æœæœ‰å…¬å¸å…§è¦ï¼Œä»¥å…¬å¸å…§è¦ç‚ºä¸»ï¼Œæ³•å¾‹è¦å®šç‚ºè¼”åŠ©åƒç…§
3. å¦‚æœå…¬å¸å…§è¦çš„è¦å®š**ä½æ–¼**å‹å‹•æ³•çš„æœ€ä½æ¨™æº–ï¼Œå¿…é ˆæ˜ç¢ºæŒ‡å‡º
4. è‹¥å…¬å¸å…§è¦é«˜æ–¼æ³•å®šæœ€ä½æ¨™æº–ï¼Œå±¬åˆæ³•ä¸”æ‡‰æ˜ç¢ºæŒ‡å‡º
5. è‹¥åƒè€ƒè³‡æ–™ä¸­å‡ºç¾ã€Œæ¸¬è©¦é™·é˜±ï¼æé†’ï¼è­¦ç¤ºã€ï¼Œéœ€ä¾å…¶å…§å®¹ä¿®æ­£çµè«–ä¸¦é»å‡ºåŸå› 
6. ä½¿ç”¨çµæ§‹åŒ–æ ¼å¼ï¼ˆæ¨™é¡Œã€æ¢åˆ—ï¼‰è®“å›ç­”æ¸…æ¥šæ˜“è®€
7. å¼•ç”¨æ³•å¾‹æ™‚ï¼Œè‹¥åƒè€ƒè³‡æ–™åŒ…å«å…·é«”æ¢è™Ÿï¼ˆå¦‚ç¬¬38æ¢ï¼‰ï¼Œ**å¿…é ˆ**å¼•ç”¨åˆ°æ¢è™Ÿï¼ˆä¾‹å¦‚ï¼šã€Šå‹å‹•åŸºæº–æ³•ã€‹ç¬¬38æ¢ï¼‰ï¼Œä¸èƒ½åªå¯«æ³•å¾‹åç¨±
8. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œå¦ç™½èªªæ˜ä¸¦å»ºè­°è«®è©¢ HR éƒ¨é–€
9. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
10. éœ€è¦æ•¸å€¼è¨ˆç®—æ™‚ï¼Œè«‹åˆ—å‡ºå…¬å¼èˆ‡ä»£å…¥å€¼ï¼Œåš´æ ¼ä¾å…¬å¼è¨ˆç®—"""

    FOLLOWUP_PROMPT = """

åœ¨å›ç­”çš„æœ€å¾Œï¼Œè«‹å¦èµ·ä¸€è¡Œè¼¸å‡º 2-3 å€‹ä½¿ç”¨è€…å¯èƒ½æœƒè¿½å•çš„å»ºè­°å•é¡Œï¼Œæ ¼å¼ï¼š
[å»ºè­°å•é¡Œ]
1. ...
2. ...
3. ..."""
    
    def __init__(self):
        self.kb_retriever = KnowledgeBaseRetriever()
        self.core_client = CoreAPIClient()

        # OpenAI client (sync + async)
        self._openai = None
        self._openai_async = None
        openai_key = getattr(settings, "OPENAI_API_KEY", "")
        if _HAS_OPENAI and openai_key:
            self._openai = openai_lib.OpenAI(api_key=openai_key)
            self._openai_async = openai_lib.AsyncOpenAI(api_key=openai_key)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T7-0: æª¢ç´¢å±¤ï¼ˆèˆ‡ç”Ÿæˆè§£è€¦ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def retrieve_context(
        self,
        tenant_id: UUID,
        question: str,
        top_k: int = 5,
    ) -> Dict[str, Any]:
        """
        ç´”æª¢ç´¢ï¼šä¸¦è¡ŒæŸ¥è©¢å…¬å¸å…§è¦ + å‹è³‡æ³• Core APIï¼Œå›å‚³çµæ§‹åŒ–ä¸Šä¸‹æ–‡ã€‚
        
        åˆ†é›¢è‡ªåŸ process_queryï¼Œä½¿ä¸²æµç«¯é»å¯å…ˆå–å¾—ä¾†æºï¼Œå†åˆ†æ®µç”Ÿæˆã€‚
        """
        request_id = str(uuid.uuid4())

        async def get_company_policy():
            try:
                # run_in_executorï¼šsearch() å«åŒæ­¥ Voyage embed/rerank å‘¼å«
                # è‹¥ç›´æ¥åœ¨ async def ä¸­å‘¼å«æœƒé˜»å¡ event loopï¼Œ
                # å°è‡´ asyncio.gather() ç„¡æ³•çœŸæ­£ä¸¦è¡Œã€‚
                loop = asyncio.get_event_loop()
                results = await loop.run_in_executor(
                    None,
                    lambda: self.kb_retriever.search(
                        tenant_id=tenant_id,
                        query=question,
                        top_k=top_k,
                    ),
                )
                return {"status": "success", "results": results}
            except Exception as e:
                return {"status": "error", "error": str(e), "results": []}

        async def get_labor_law():
            try:
                result = await self.core_client.chat(
                    question=question,
                    request_id=request_id,
                )
                return result
            except Exception as e:
                return {"status": "error", "answer": "å‹è³‡æ³•æŸ¥è©¢å¤±æ•—", "error": str(e)}

        company_policy_result, labor_law_result = await asyncio.gather(
            asyncio.create_task(get_company_policy()),
            asyncio.create_task(get_labor_law()),
        )

        # å…§è¦è£œå¼·ï¼šæ ¹æ“šå•é¡Œé—œéµå­—åšæª”åå°å‘æª¢ç´¢ï¼ˆåŒæ¨£ç”¨ executor é¿å…é˜»å¡ï¼‰
        loop = asyncio.get_event_loop()
        boosted_results = await loop.run_in_executor(
            None,
            lambda: self._policy_boost_search(tenant_id, question, top_k),
        )
        if boosted_results:
            base_results = company_policy_result.get("results", [])
            merged = self._merge_policy_results(base_results, boosted_results, top_k)
            company_policy_result["status"] = "success"
            company_policy_result["results"] = merged

        # â”€â”€ çµ„è£çµæ§‹åŒ–ä¸Šä¸‹æ–‡ â”€â”€
        return self._build_context(
            question=question,
            company_policy=company_policy_result,
            labor_law=labor_law_result,
            request_id=request_id,
        )

    def _policy_boost_search(
        self, tenant_id: UUID, question: str, top_k: int
    ) -> List[Dict[str, Any]]:
        filenames = self._policy_hint_filenames(question)
        if not filenames:
            return []
        try:
            return self.kb_retriever.search(
                tenant_id=tenant_id,
                query=question,
                top_k=top_k,
                mode="semantic",
                rerank=False,
                filter_dict={"filename": filenames},
            )
        except Exception:
            return []

    @staticmethod
    def _policy_hint_filenames(question: str) -> List[str]:
        hints: List[str] = []
        if any(k in question for k in ["ç¸¾æ•ˆ", "è€ƒæ ¸"]):
            hints.append("å“¡å·¥æ‰‹å†Š-ç¬¬ä¸€ç« -ç¸½å‰‡.pdf")
        if any(k in question for k in ["å ±å¸³", "è¨ˆç¨‹è»Š", "æ†‘è­‰", "ç™¼ç¥¨"]):
            hints.append("å ±å¸³ä½œæ¥­è¦ç¯„.pdf")
        if any(k in question for k in ["æ–°äºº", "å ±åˆ°", "åˆ°è·", "è©¦ç”¨æœŸ"]):
            hints.extend(["æ–°äººåˆ°è·SOP.pdf", "å‹å‹•å¥‘ç´„æ›¸-è¬é›…ç².pdf"])
        if any(k in question for k in ["ç‰¹ä¼‘", "å©šå‡", "å–ªå‡", "ç”Ÿç†å‡", "ç”¢å‡", "é™ªç”¢", "è«‹å‡"]):
            hints.extend(["å“¡å·¥æ‰‹å†Š-ç¬¬ä¸€ç« -ç¸½å‰‡.pdf", "è«‹å‡å–®ç¯„æœ¬-E012-å‘¨ç§€è˜­.pdf"])
        if "å¹´çµ‚çé‡‘" in question or "çæ‡²" in question:
            hints.extend(["çæ‡²ç®¡ç†è¾¦æ³•.pdf", "å‹å‹•å¥‘ç´„æ›¸-è¬é›…ç².pdf"])
        if "åŠ ç­" in question:
            hints.extend(["å“¡å·¥æ‰‹å†Š-ç¬¬ä¸€ç« -ç¸½å‰‡.pdf", "å‹å‹•å¥‘ç´„æ›¸-è¬é›…ç².pdf"])
        if "äº¤é€šæ´¥è²¼" in question or "æ´¥è²¼" in question:
            hints.append("å“¡å·¥æ‰‹å†Š-ç¬¬ä¸€ç« -ç¸½å‰‡.pdf")
        if "å‹ä¿" in question or "å¥ä¿" in question:
            hints.append("202601-E007-åŠ‰å¿—æ˜-è–ªè³‡æ¢.pdf")
        if "å¥æª¢" in question or "å¥åº·æª¢æŸ¥" in question:
            hints.append("å¥åº·æª¢æŸ¥å ±å‘Š-E016-é«˜æ·‘ç.pdf")
        if "è–ªè³‡" in question or "è–ªæ°´" in question or "å¯¦é ˜" in question:
            hints.append("202601-E007-åŠ‰å¿—æ˜-è–ªè³‡æ¢.pdf")
        # å»é‡ä¿æŒé †åº
        seen = set()
        ordered = []
        for name in hints:
            if name not in seen:
                seen.add(name)
                ordered.append(name)
        return ordered

    @staticmethod
    def _merge_policy_results(
        base: List[Dict[str, Any]],
        extra: List[Dict[str, Any]],
        max_results: int,
    ) -> List[Dict[str, Any]]:
        seen = set()
        merged: List[Dict[str, Any]] = []
        for item in extra + base:
            key = item.get("id") or f"{item.get('document_id')}:{item.get('chunk_index')}"
            if key in seen:
                continue
            seen.add(key)
            merged.append(item)
            if len(merged) >= max_results:
                break
        return merged

    def _build_context(
        self,
        question: str,
        company_policy: Dict[str, Any],
        labor_law: Dict[str, Any],
        request_id: str,
    ) -> Dict[str, Any]:
        """å°‡ raw æª¢ç´¢çµæœçµ„è£ç‚ºçµæ§‹åŒ– context dictã€‚"""
        has_policy = (
            company_policy.get("status") == "success"
            and len(company_policy.get("results", [])) > 0
        )
        has_labor_law = (
            labor_law.get("status") != "error" and labor_law.get("answer")
        )

        context: Dict[str, Any] = {
            "request_id": request_id,
            "question": question,
            "has_policy": has_policy,
            "has_labor_law": has_labor_law,
            "company_policy_raw": None,
            "labor_law_raw": None,
            "context_parts": [],
            "sources": [],
            "disclaimer": "æœ¬å›ç­”åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ­£å¼æ³•å¾‹æ„è¦‹ã€‚å¦‚æœ‰å…·é«”æƒ…æ³ï¼Œè«‹è«®è©¢å°ˆæ¥­æ³•å¾‹é¡§å•ã€‚",
        }

        if has_policy:
            top_policies = company_policy["results"][:3]
            context["company_policy_raw"] = {
                "content": top_policies[0].get("content") or "",
                "source": top_policies[0].get("filename") or "",
                "relevance_score": top_policies[0].get("score") or 0,
                "all_results": [
                    {
                        "content": (r.get("content") or "")[:500],
                        "filename": r.get("filename") or "",
                        "score": r.get("score") or 0,
                    }
                    for r in top_policies
                ],
            }
            for r in top_policies:
                context["sources"].append({
                    "type": "policy",
                    "title": r.get("filename") or "",
                    "snippet": (r.get("content") or "")[:200],
                    "score": r.get("score") or 0,
                })
            for i, r in enumerate(top_policies, 1):
                content = r.get("content") or ""
                filename = r.get("filename") or ""
                score = r.get("score") or 0
                context["context_parts"].append(
                    f"ã€å…¬å¸å…§è¦ #{i}ã€‘ï¼ˆä¾†æºï¼š{filename}ï¼Œç›¸é—œåº¦ï¼š{score:.2f}ï¼‰\n{content}"
                )

        if has_labor_law:
            context["labor_law_raw"] = {
                "answer": labor_law.get("answer", ""),
                "citations": labor_law.get("citations", []),
                "usage": labor_law.get("usage", {}),
            }
            if labor_law.get("citations"):
                for citation in labor_law["citations"]:
                    law_name = citation.get("law_name") or "å‹å‹•æ³•è¦"
                    article = citation.get("article") or ""
                    title = f"{law_name} {article}".strip()
                    context["sources"].append({
                        "type": "law",
                        "title": title,
                        "snippet": labor_law.get("answer", "")[:200],
                    })
            else:
                # Core API ä¸å›å‚³çµæ§‹åŒ– citationsï¼Œå¾å›ç­”æ–‡å­—ä¸­è§£ææ³•æ¢å¼•ç”¨
                answer_text = labor_law.get("answer") or ""
                if answer_text:
                    law_refs = re.findall(r'ã€Š(.+?)ã€‹(?:ç¬¬(\d+[-ä¹‹]?\d*æ¢?))?', answer_text)
                    if law_refs:
                        seen = set()
                        for law_name, article in law_refs[:5]:
                            key = f"{law_name} {article}".strip()
                            if key not in seen:
                                seen.add(key)
                                context["sources"].append({
                                    "type": "law",
                                    "title": key,
                                    "snippet": answer_text[:200],
                                })
                    else:
                        context["sources"].append({
                            "type": "law",
                            "title": "å‹å‹•æ³•è¦ (Core API)",
                            "snippet": answer_text[:200],
                        })
            law_text = labor_law.get("answer", "")
            citations_text = ""
            if labor_law.get("citations"):
                citations_text = "ï¼›".join(
                    f"{c.get('law_name', '')} {c.get('article', '')}"
                    for c in labor_law["citations"]
                )
            elif law_text:
                # Core API ä¸å›å‚³çµæ§‹åŒ– citationsï¼Œå¾ answer æ–‡å­—è§£ææ³•æ¢åšç‚º heading
                parsed = re.findall(r'ã€Š(.+?)ã€‹(?:ç¬¬([\d\-ä¹‹]+æ¢(?:ä¹‹\d+)?))?', law_text)
                seen_cit: set = set()
                unique_cit: list = []
                for law_n, art_n in parsed[:8]:
                    key = f"ã€Š{law_n}ã€‹ç¬¬{art_n}æ¢" if art_n else f"ã€Š{law_n}ã€‹"
                    if key not in seen_cit:
                        seen_cit.add(key)
                        unique_cit.append(key)
                if unique_cit:
                    citations_text = "ï¼ˆæ³•æºï¼š" + "ã€".join(unique_cit) + "ï¼‰"
            context["context_parts"].append(
                f"ã€å‹å‹•æ³•è¦ã€‘{citations_text}\n{law_text}"
            )

        return context

    async def stream_answer(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
        include_followup: bool = True,
    ) -> AsyncGenerator[str, None]:
        """
        ä¸²æµç”Ÿæˆ LLM å›ç­”ï¼ˆSSE ç”¨ï¼‰ã€‚

        yield æ¯å€‹ token chunkï¼Œå‰ç«¯å¯é€å­—æ¸²æŸ“ã€‚
        è‹¥ LLM ä¸å¯ç”¨ï¼Œå‰‡ yield æ•´æ®µ fallbackã€‚
        """
        if not self._openai_async or not (context["has_policy"] or context["has_labor_law"]):
            yield self._fallback_answer(context)
            return

        messages = self._build_llm_messages(
            question, context, history=history, include_followup=include_followup
        )

        try:
            response = await self._openai_async.chat.completions.create(
                model=getattr(settings, "OPENAI_MODEL", "gpt-4o-mini"),
                messages=messages,
                temperature=getattr(settings, "OPENAI_TEMPERATURE", 0.3),
                max_tokens=getattr(settings, "OPENAI_MAX_TOKENS", 1500),
                stream=True,
            )
            async for chunk in response:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content
        except Exception as e:
            logger.warning(f"LLM ä¸²æµç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°æ¨¡æ¿: {e}")
            yield self._fallback_answer(context)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T7-2: å¤šè¼ªå°è©±æ”¯æ´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # éœ€è¦ä¸Šä¸‹æ–‡è£œå…¨çš„ä»£åè©ï¼æŒ‡ç¤ºè©
    _CONTEXT_PRONOUNS = ("ä»–", "å¥¹", "å®ƒ", "ä»–çš„", "å¥¹çš„", "ä»–å€‘", "å¥¹å€‘",
                         "é€™å€‹äºº", "é‚£å€‹äºº", "æ­¤äºº", "è©²å“¡å·¥", "åŒä¸€", "ä¸Šè¿°", "å‰è¿°")

    async def contextualize_query(
        self, query: str, history: List[Dict[str, str]]
    ) -> str:
        """
        ç”¨ LLM å°‡å«ä»£åè©/çœç•¥ä¸»è©çš„æŸ¥è©¢æ”¹å¯«ç‚ºç¨ç«‹æŸ¥è©¢ã€‚
        è‹¥æ­·å²ç‚ºç©ºã€LLM ä¸å¯ç”¨ã€æˆ–å•é¡Œä¸å«æŒ‡ä»£è©ï¼Œç›´æ¥å›å‚³åŸ queryã€‚
        """
        if not history or not self._openai_async:
            return query

        # æ™ºæ…§è·³éï¼šå•é¡Œä¸å«ä»£åè©/æŒ‡ç¤ºè©æ™‚ç„¡éœ€ LLM æ”¹å¯«ï¼ˆç¯€çœ ~0.9sï¼‰
        if not any(p in query for p in self._CONTEXT_PRONOUNS):
            return query

        messages = [
            {
                "role": "system",
                "content": (
                    "æ ¹æ“šå°è©±æ­·å²ï¼Œå°‡ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œæ”¹å¯«ç‚ºä¸€å€‹ç¨ç«‹ã€å®Œæ•´çš„æŸ¥è©¢ã€‚"
                    "åªè¼¸å‡ºæ”¹å¯«å¾Œçš„æŸ¥è©¢ï¼Œä¸è¦è§£é‡‹ã€‚å¦‚æœå•é¡Œå·²ç¶“å¤ æ˜ç¢ºï¼Œç›´æ¥åŸæ¨£è¼¸å‡ºã€‚"
                ),
            },
            *[{"role": m["role"], "content": m["content"]} for m in history[-4:]],
            {"role": "user", "content": query},
        ]

        try:
            response = await self._openai_async.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0,
                max_tokens=200,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.warning(f"æŸ¥è©¢æ”¹å¯«å¤±æ•—: {e}")
            return query

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å‘ä¸‹ç›¸å®¹ï¼šä¿ç•™åŸ process_query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def process_query(
        self,
        tenant_id: UUID,
        question: str,
        top_k: int = settings.RETRIEVAL_TOP_K,
        conversation_id: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
    ) -> Dict[str, Any]:
        """
        è™•ç†ç”¨æˆ¶æŸ¥è©¢ï¼ˆéä¸²æµï¼Œå‘ä¸‹ç›¸å®¹ï¼‰ã€‚
        
        æ–°å¢ conversation_id / history åƒæ•¸ä»¥æ”¯æ´å¤šè¼ªå°è©±ã€‚
        """
        structured = try_structured_answer(tenant_id, question, history=history)
        if structured:
            return {
                "request_id": str(uuid.uuid4()),
                "question": question,
                "company_policy": None,
                "labor_law": None,
                "answer": structured.answer,
                "sources": structured.sources,
                "notes": ["ä½¿ç”¨çµæ§‹åŒ–è³‡æ–™ç›´æ¥è¨ˆç®—"],
                "disclaimer": "æœ¬å›ç­”åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæ­£å¼æ³•å¾‹æ„è¦‹ã€‚å¦‚æœ‰å…·é«”æƒ…æ³ï¼Œè«‹è«®è©¢å°ˆæ¥­æ³•å¾‹é¡§å•ã€‚",
            }
        # æŸ¥è©¢æ”¹å¯«ï¼ˆå¤šè¼ªï¼‰
        effective_question = question
        if history:
            effective_question = await self.contextualize_query(question, history)

        # æª¢ç´¢
        ctx = await self.retrieve_context(
            tenant_id=tenant_id,
            question=effective_question,
            top_k=top_k,
        )

        # ç”Ÿæˆå›ç­”ï¼ˆéä¸²æµï¼‰
        result = {
            "request_id": ctx["request_id"],
            "question": question,
            "company_policy": ctx["company_policy_raw"],
            "labor_law": ctx["labor_law_raw"],
            "answer": "",
            "sources": ctx["sources"],
            "notes": [],
            "disclaimer": ctx["disclaimer"],
        }

        if self._openai and (ctx["has_policy"] or ctx["has_labor_law"]):
            try:
                result["answer"] = self._generate_answer_sync(
                    question, ctx, history=history
                )
                result["notes"].append("ç”± AI æ ¹æ“šæª¢ç´¢çµæœç”Ÿæˆå›ç­”")
            except Exception as e:
                logger.warning(f"LLM å›ç­”ç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°æ¨¡æ¿: {e}")
                result["answer"] = self._fallback_answer(ctx)
                result["notes"].append("LLM æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œä»¥çµæ§‹åŒ–æ ¼å¼å‘ˆç¾")
        else:
            result["answer"] = self._fallback_answer(ctx)
            if not (ctx["has_policy"] or ctx["has_labor_law"]):
                result["notes"].append("æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š")

        return result

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLM Messages çµ„è£ï¼ˆå…±ç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_llm_messages(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
        include_followup: bool = True,
    ) -> List[Dict[str, str]]:
        """çµ„è£ LLM çš„ messages é™£åˆ—ï¼ˆå«æ­·å² + æª¢ç´¢ä¸Šä¸‹æ–‡ï¼‰ã€‚"""
        system_content = self.SYSTEM_PROMPT
        if include_followup:
            system_content += self.FOLLOWUP_PROMPT

        messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_content}
        ]

        # æ³¨å…¥æ­·å²ï¼ˆToken é ç®—ç®¡ç†ï¼‰
        if history:
            max_history_tokens = 2000
            total_tokens = 0
            history_msgs = []
            for msg in reversed(history):
                # ç²—ä¼° 1 ä¸­æ–‡å­— â‰ˆ 2 tokens
                msg_tokens = len(msg["content"])
                if total_tokens + msg_tokens > max_history_tokens:
                    break
                history_msgs.insert(0, {"role": msg["role"], "content": msg["content"]})
                total_tokens += msg_tokens
            messages.extend(history_msgs)

        context_text = "\n\n".join(context["context_parts"])
        history_summary = self._format_history_summary(history)
        calc_guidance = self._build_calc_guidance(question)
        user_content = f"å•é¡Œï¼š{question}\n\nåƒè€ƒè³‡æ–™ï¼š\n{context_text}\n\nè«‹æ ¹æ“šä¸Šè¿°åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚"
        if history_summary:
            user_content = f"å°è©±æ­·å²æ‘˜è¦ï¼š\n{history_summary}\n\n" + user_content
        if calc_guidance:
            user_content += f"\n\nè¨ˆç®—èˆ‡åˆ¤æ–·æç¤ºï¼š\n{calc_guidance}"
        messages.append({"role": "user", "content": user_content})

        return messages

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŒæ­¥ç”Ÿæˆï¼ˆç›¸å®¹åŸä»‹é¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _generate_answer_sync(
        self,
        question: str,
        context: Dict[str, Any],
        history: Optional[List[Dict[str, str]]] = None,
    ) -> str:
        """åŒæ­¥ LLM ç”Ÿæˆå›ç­”ï¼ˆéä¸²æµï¼‰ã€‚"""
        messages = self._build_llm_messages(question, context, history=history)

        response = self._openai.chat.completions.create(
            model=getattr(settings, "OPENAI_MODEL", "gpt-4o-mini"),
            messages=messages,
            temperature=getattr(settings, "OPENAI_TEMPERATURE", 0.3),
            max_tokens=getattr(settings, "OPENAI_MAX_TOKENS", 1500),
        )
        return response.choices[0].message.content.strip()

    @staticmethod
    def _build_calc_guidance(question: str) -> str:
        hints: List[str] = []
        if "ç‰¹ä¼‘" in question or "ç‰¹åˆ¥ä¼‘å‡" in question:
            hints.append("ç‰¹ä¼‘å¤©æ•¸ä¾å‹åŸºæ³•ç¬¬38æ¢ï¼ŒæŒ‰ã€å¯¦éš›åˆ°è·æ—¥ã€è¨ˆç®—å¹´è³‡ï¼Œè€Œéå•é¡Œæ•˜è¿°ä¸­çš„æ¦‚ç®—ã€‚")
            hints.append("å¹´è³‡å€é–“ï¼šæœªæ»¿6å€‹æœˆ=0å¤©ï¼Œ6å€‹æœˆä»¥ä¸Šæœªæ»¿1å¹´=3å¤©ï¼Œ1å¹´=7å¤©ï¼Œ2å¹´=10å¤©ï¼Œ3å¹´=14å¤©ï¼Œ5å¹´=15å¤©ï¼Œ10å¹´ä»¥ä¸Šæ¯å¹´+1å¤©(æœ€å¤š30å¤©)ã€‚")
            hints.append("è‹¥å•é¡Œå«æœ‰å…·é«”åˆ°è·æ—¥æœŸï¼Œè«‹è¨ˆç®—åˆ°ä»Šå¤©ï¼ˆ2026å¹´2æœˆ23æ—¥ï¼‰çš„æ­£ç¢ºå¹´è³‡å¾Œå†æŸ¥å°ç…§è¡¨ã€‚")
        if "è³‡é£è²»" in question:
            hints.append("è³‡é£è²»å…¬å¼ï¼šå¹´è³‡(å¹´) Ã— 0.5 Ã— æœˆå¹³å‡å·¥è³‡ã€‚ä¸è¦æŠŠæœˆè–ªé™¤ä»¥30ã€‚")
            hints.append("å¹´è³‡è‹¥å«æœˆä»½ï¼Œéœ€æ›ç®—ç‚ºå¹´ä¸¦å¯å››æ¨äº”å…¥åˆ° 0.5 å¹´å†è¨ˆç®—ã€‚")
        if "åŠ ç­" in question:
            hints.append("æ™‚è–ªè¨ˆç®—ï¼šæ™‚è–ª = æœˆè–ª / 30 / 8ï¼ˆå‹åŸºæ³•åŸºæº–ï¼‰ã€‚")
            hints.append("å¹³æ—¥åŠ ç­è²»ï¼šå‰ 2 å°æ™‚æ¯å°æ™‚ Ã— 1.34 å€ï¼Œç¬¬ 3 å°æ™‚èµ·æ¯å°æ™‚ Ã— 1.67 å€ã€‚")
            hints.append("ä¼‘æ¯æ—¥åŠ ç­è²»ï¼šå‰ 2 å°æ™‚æ¯å°æ™‚ Ã— 1.34 å€ï¼Œç¬¬ 3-8 å°æ™‚æ¯å°æ™‚ Ã— 1.67 å€ï¼Œç¬¬ 9 å°æ™‚èµ· Ã— 2.67 å€ã€‚")
            hints.append("è¨ˆç®—æ™‚å¿…é ˆåˆ†æ®µè¨ˆç®—ï¼Œä¸å¯æŠŠå…¨éƒ¨æ™‚æ•¸éƒ½ä¹˜åŒä¸€å€ç‡ã€‚ä¾‹å¦‚ï¼šå¹³æ—¥åŠ ç­ 4 å°æ™‚ = å‰ 2 å°æ™‚ Ã— 1.34 + å¾Œ 2 å°æ™‚ Ã— 1.67ã€‚")
        if "å¹³å‡" in question and ("è–ª" in question or "æœˆè–ª" in question):
            hints.append("å¹³å‡å€¼éœ€ä½¿ç”¨æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„è³‡æ–™åˆ—ï¼Œä¸è¦åªå–å‰å¹¾ç­†ã€‚")
        if "å æ¯”" in question or "æ¯”ä¾‹" in question:
            hints.append("çµ±è¨ˆé¡Œè«‹é€ä¸€è¨ˆæ•¸ä¸¦æ ¸å°ç¸½æ•¸å¾Œå†è¨ˆç®—æ¯”ä¾‹ã€‚")
        if "å¹´è³‡æœ€æ·±" in question or ("æœ€æ·±" in question and "å¹´è³‡" in question):
            hints.append("æœ€æ·±å¹´è³‡éœ€æ¯”å°å®Œæ•´åå†Šå¾Œå†ä¸‹çµè«–ã€‚")
        if "åŠ ç­" in question and ("åˆæ³•" in question or "åˆæ³•å—" in question):
            hints.append("è‹¥é¡Œç›®åªçµ¦å–®ä¸€å€æ•¸ï¼ˆå¦‚ 1.5 å€ï¼‰ï¼Œè¦–ç‚ºå‰ 2 å°æ™‚æ¨™æº–ï¼›å¯åˆ¤å®šåˆæ³•ï¼Œä½†æé†’è¶…é 2 å°æ™‚éœ€ 1.67 å€ã€‚")
        if "å‹ä¿" in question:
            hints.append("è‹¥è–ªè³‡æ¢å·²åˆ—å‡ºå‹ä¿è‡ªä»˜é‡‘é¡ï¼Œç›´æ¥å¼•ç”¨è©²æ•¸å€¼ã€‚")
        if "é¢±é¢¨" in question or "åœç­åœèª²" in question:
            hints.append("é¢±é¢¨åœç­åœèª²å±¬è¡Œæ”¿å»ºè­°æ€§è³ªï¼Œé›‡ä¸»å¯è¦–éœ€è¦å‡ºå‹¤ï¼Œä½†ä¸å¾—ä¸åˆ©è™•åˆ†ï¼›è‹¥å‡ºå‹¤éœ€ä¾è¦å®šçµ¦ä»˜ã€‚")
        if "è²¬ä»»åˆ¶" in question:
            hints.append("ä¸€èˆ¬å·¥ç¨‹å¸«é€šå¸¸ä¸é©ç”¨è²¬ä»»åˆ¶ï¼Œä»æ‡‰ä¾å·¥æ™‚è¦å®šèˆ‡åŠ ç­è²»è¦å®šã€‚")
        if "å¹´çµ‚çé‡‘" in question and "å·¥è³‡" in question:
            hints.append("å¹´çµ‚çé‡‘æ˜¯å¦å±¬å·¥è³‡éœ€è¦–æ˜¯å¦ç‚ºç¶“å¸¸æ€§/å›ºå®šæ€§çµ¦ä»˜èˆ‡å¥‘ç´„ç´„å®šï¼Œé€šå¸¸éœ€å€‹æ¡ˆåˆ¤æ–·ã€‚")
        if "é›¢è·" in question and "è³‡é£è²»" in question:
            hints.append("è‡ªè«‹é›¢è·ç„¡è³‡é£è²»ï¼›è³‡é£è²»åƒ…é©ç”¨é›‡ä¸»ä¾æ³•è³‡é£æƒ…æ³ã€‚")
        if "å–ªå‡" in question and "é…å¶" in question and "ç¥–çˆ¶æ¯" in question:
            hints.append("é…å¶çš„ç¥–çˆ¶æ¯å–ªå‡æ³•å®š 3 å¤©ï¼›å¦‚å…¬å¸å…§è¦çµ¦æ›´é«˜å¤©æ•¸å¯è¦–ç‚ºå„ªæ–¼æ³•ä»¤ã€‚")
        if not hints:
            return ""
        return "\n".join(f"- {h}" for h in hints)

    @staticmethod
    def _format_history_summary(history: Optional[List[Dict[str, str]]]) -> str:
        if not history:
            return ""
        kept = history[-2:]
        lines = []
        for msg in kept:
            role = msg.get("role", "user")
            content = msg.get("content", "").strip()
            if not content:
                continue
            lines.append(f"[{role}] {content[:200]}")
        return "\n".join(lines)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @staticmethod
    def _fallback_answer(context: Dict[str, Any]) -> str:
        """LLM ä¸å¯ç”¨æ™‚çš„æ¨¡æ¿ fallbackã€‚"""
        has_policy = context.get("has_policy", False)
        has_labor_law = context.get("has_labor_law", False)

        if has_policy and has_labor_law:
            policy_content = context["company_policy_raw"]["content"][:500]
            law_answer = context["labor_law_raw"]["answer"][:500]
            return f"""ğŸ“‹ **å…¬å¸å…§è¦è¦å®š**ï¼š
{policy_content}

âš–ï¸ **å‹å‹•æ³•è¦è£œå……**ï¼š
{law_answer}

ğŸ’¡ **èªªæ˜**ï¼šå…¬å¸å…§è¦æ˜¯æ‚¨çš„å„ªå…ˆåƒè€ƒä¾æ“šï¼Œä½†ä¸å¾—é•åå‹å‹•æ³•çš„æœ€ä½æ¨™æº–ã€‚"""

        elif has_policy:
            return f"""ğŸ“‹ **å…¬å¸å…§è¦è¦å®š**ï¼š
{context["company_policy_raw"]["content"]}

ğŸ’¡ **æé†’**ï¼šæœªæŸ¥è©¢åˆ°ç›¸é—œå‹å‹•æ³•è¦è£œå……ã€‚å¦‚éœ€äº†è§£æ³•å¾‹æœ€ä½æ¨™æº–ï¼Œè«‹é€²ä¸€æ­¥è«®è©¢ã€‚"""

        elif has_labor_law:
            return f"""âš–ï¸ **å‹å‹•æ³•è¦**ï¼š
{context["labor_law_raw"]["answer"]}

ğŸ’¡ **æé†’**ï¼šæœªåœ¨å…¬å¸å…§è¦ä¸­æ‰¾åˆ°ç›¸é—œè¦å®šã€‚å»ºè­°ç¢ºèªå…¬å¸æ˜¯å¦æœ‰é¡å¤–è¦å®šã€‚"""

        else:
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚è«‹å˜—è©¦æ›å€‹æ–¹å¼æå•ï¼Œæˆ–è¯ç¹« HR éƒ¨é–€ã€‚"

    def format_summary(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ‘˜è¦ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰"""
        summary = f"**å•é¡Œ**ï¼š{result['question']}\n\n"
        summary += result["answer"]

        if result["sources"]:
            summary += "\n\n**åƒè€ƒä¾†æº**ï¼š\n"
            for source in result["sources"]:
                if source["type"] == "company_policy":
                    summary += f"- ğŸ“‹ {source['filename']} (ç›¸é—œåº¦: {source['score']:.2f})\n"
                elif source["type"] == "labor_law":
                    summary += f"- âš–ï¸ {source.get('law_name', 'å‹å‹•æ³•è¦')} {source.get('article', '')}\n"

        summary += f"\n\n{result['disclaimer']}"
        return summary
