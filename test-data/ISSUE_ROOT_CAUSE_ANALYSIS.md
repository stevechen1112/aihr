# UniHR SaaS â€” å•é¡Œæ ¹å› åˆ†æèˆ‡ä¿®å¾©æ–¹æ¡ˆ

> ç”Ÿæˆæ™‚é–“: 2026-02-10T16:25  
> æ¸¬è©¦ç’°å¢ƒ: 172 ä¼ºæœå™¨ (Docker 6 containers)

---

## æ‘˜è¦

æ¸¬è©¦è©•åˆ† 86/129 (66.7%)ï¼Œä¸ä»£è¡¨ç³»çµ±æœ‰å¤§é‡ç¨‹å¼ç¢¼ç¼ºé™·ã€‚  
ç¶“éå®Œæ•´è¿½è¹¤ï¼Œ**çœŸæ­£çš„æ ¹å› åªæœ‰ 3 å€‹**ï¼Œå…¶é¤˜éƒ½æ˜¯é€£é–åæ‡‰ï¼š

| # | æ ¹å›  | å½±éŸ¿ç¯„åœ | åš´é‡åº¦ | ç‹€æ…‹ |
|---|------|---------|-------|------|
| 1 | Celery Worker ä»»å‹™æœªè¨»å†Š + ä½‡åˆ—åä¸ç¬¦ | æ‰€æœ‰æ–‡ä»¶è™•ç† â†’ æ‰€æœ‰å…¬å¸å…§è¦å•ç­” | ğŸ”´ Critical | âœ… å·²ä¿® |
| 2 | è³‡æ–™åº« Schema ä¸åŒæ­¥ (é·ç§»æœªå®Œæ•´) | ä¸Šå‚³ 500, æª¢ç´¢å¤±æ•— | ğŸ”´ Critical | âœ… å·²ä¿® |
| 3 | Core API å›æ‡‰æ ¼å¼ä¸åŒ…å«çµæ§‹åŒ– `citations` | æ³•è¦ä¾†æº sources ç‚ºç©º â†’ è©•åˆ†åªå¾— 2/3 | ğŸŸ¡ Medium | â¬œ å¾…ä¿® |

---

## å•é¡Œä¸€è¦½ï¼ˆå«å·²ä¿®å¾©é …ç›®ï¼‰

---

### ğŸ”§ å•é¡Œ A: æ–‡ä»¶ä¸Šå‚³å¾Œå…¨éƒ¨åœåœ¨ "uploading" ç‹€æ…‹ï¼ˆå·²ä¿®å¾©ï¼‰

**ç¾è±¡**: 11 ä»½æ–‡ä»¶ä¸Šå‚³æˆåŠŸ (HTTP 200)ï¼Œä½† Worker å¾æœªè™•ç†

**æ ¹å› **:  
1. `celery_app.py` ç¼ºå°‘ `import app.tasks.document_tasks` â†’ Celery å•Ÿå‹•æ™‚æœªè¨»å†Šä»»ä½• task  
2. `celery_app.py` çš„ task_routes è¨­å®šä½‡åˆ—ç‚º `"default"`ï¼Œä½† Celery é è¨­ç›£è½ `"celery"` â†’ å³ä½¿æ‰‹å‹•è¨»å†Šäº† taskï¼Œæ¶ˆæ¯ä¹Ÿä¸æœƒè¢«æ¶ˆè²»

**ä¿®å¾©** (å·²å®Œæˆ):  
```python
# celery_app.py
celery_app.autodiscover_tasks(['app.tasks'])
import app.tasks.document_tasks

# task_routes queue æ”¹ç‚º "celery"
task_routes = {'app.tasks.*': {'queue': 'celery'}}
```

**é©—è­‰**: ä¸Šå‚³ hr-policy-test.md â†’ Worker æˆåŠŸè™•ç† â†’ 4 chunks å¯«å…¥ pgvector âœ…

---

### ğŸ”§ å•é¡Œ B: æ‰€æœ‰ä¸Šå‚³è¿”å› HTTP 500ï¼ˆå·²ä¿®å¾©ï¼‰

**ç¾è±¡**: POST /api/v1/documents/upload â†’ 500 Internal Server Error

**æ ¹å› **: `documents` è¡¨ç¼ºå°‘ 3 å€‹æ¬„ä½ (`file_size`, `chunk_count`, `quality_report`)  
SQLAlchemy Model å®šç¾©äº†é€™äº›æ¬„ä½ï¼Œä½† Alembic é·ç§»åªå»ºäº†åŸºæœ¬æ¬„ä½ã€‚INSERT æ™‚å˜—è©¦å¯«å…¥ä¸å­˜åœ¨çš„æ¬„ä½ â†’ ProgrammingError

**ä¿®å¾©** (å·²å®Œæˆ):  
```sql
ALTER TABLE documents ADD COLUMN file_size BIGINT;
ALTER TABLE documents ADD COLUMN chunk_count INTEGER;
ALTER TABLE documents ADD COLUMN quality_report JSONB;
ALTER TABLE documentchunks ADD COLUMN vector_id VARCHAR(255);
```

**å…¶ä»– Schema ä¿®å¾©** (å·²å®Œæˆ):  
- `tenants` è¡¨æ–°å¢ 15 å€‹æ¬„ä½ (SSO, branding, domain ç­‰)
- å»ºç«‹ 4 å€‹ç¼ºå¤±çš„è¡¨ (chat_feedbacks, customdomains, quotaalerts, tenantsecurityconfigs)

---

### âš ï¸ å•é¡Œ C: æ‰€æœ‰å…¬å¸å…§è¦å•é¡Œç„¡æ³•å›ç­”ï¼ˆå·²æ‰¾åˆ°æ ¹å› ï¼‰

**ç¾è±¡**: æ¸¬è©¦ä¸­ "å…¬å¸ç‰¹ä¼‘å‡å¹¾å¤©ï¼Ÿ" "å¹´çµ‚çé‡‘è¦å®šï¼Ÿ" ç­‰å•é¡Œå…¨éƒ¨é  Core API (å‹å‹•æ³•è¦) å›ç­”ï¼Œç„¡æ³•å¼•ç”¨å…¬å¸å…§è¦

**æ ¹å› **: é€™æ˜¯**å•é¡Œ A çš„é€£é–åæ‡‰**ã€‚  
å›  Worker å¾æœªè™•ç†æ–‡ä»¶ â†’ `documentchunks` è¡¨ç‚ºç©º â†’ æœ¬åœ° KB retriever (`kb_retrieval.py`) æœå°‹çµæœç‚ºé›¶ â†’ ChatOrchestrator åªèƒ½ä¾è³´ Core API å›ç­”

**é©—è­‰**: ä¿®å¾© Worker å¾Œä¸Šå‚³ hr-policy-test.md â†’ 4 chunks å¯«å…¥ â†’ å•ã€Œå¹´çµ‚çé‡‘è¦å®šã€â†’ å›ç­”æ­£ç¢ºå¼•ç”¨å…¬å¸å…§è¦ (score=0.77) âœ…

**éœ€è¦çš„å‹•ä½œ**: é‡æ–°ä¸Šå‚³å…¨éƒ¨ 20 ä»½æ¸¬è©¦æ–‡ä»¶ï¼ŒPipeline å·²æ­£å¸¸

---

### âš ï¸ å•é¡Œ D: è‡ªå‹•è©•åˆ†ç¸½æ˜¯ 2/3ï¼Œå¾ä¸å¾— 3/3

**ç¾è±¡**: æ¯é¡Œæœ€é«˜å¾—åˆ†éƒ½æ˜¯ 2/3ï¼Œå¾ä¸åˆ°æ»¿åˆ†

**æ ¹å› **: è©•åˆ†é‚è¼¯ç‚ºï¼š
```python
auto = 0
if st == 200 and answer:
    auto = 1                        # æœ‰å›ç­”
    if len(answer) > 50: auto = 2   # å›ç­”å¤ é•·
    if sources: auto = min(auto+1, 3)  # æœ‰ sources â†’ 3 åˆ†
```

è¦å¾— 3 åˆ†ï¼Œ`sources` åˆ—è¡¨å¿…é ˆéç©ºã€‚Sources ä¾†è‡ªå…©è™•ï¼š
1. **å…¬å¸å…§è¦** â†’ `context["sources"].append({"type": "policy", ...})` â€” éœ€è¦æœ¬åœ° KB æœ‰è³‡æ–™
2. **å‹å‹•æ³•è¦** â†’ `context["sources"].append({"type": "law", ...})` â€” éœ€è¦ Core API å›å‚³ `citations` æ¬„ä½

**å•é¡Œ**: Core API (unihr) å›æ‡‰æ ¼å¼ç‚ºï¼š
```json
{
  "answer": "ä¾æ“šã€Šå‹å·¥è«‹å‡è¦å‰‡ã€‹ç¬¬1æ¢...ï¼ˆç›¸é—œæ³•æºï¼šã€Šå‹å‹•åŸºæº–æ³•ã€‹ç¬¬37æ¢...ï¼‰",
  "history": [...],
  "session_id": "..."
}
```
âš ï¸ **æ²’æœ‰ `citations` æ¬„ä½ï¼** æ³•è¦å¼•ç”¨å…§åµŒåœ¨ `answer` æ–‡å­—ä¸­ã€‚

ä½† ChatOrchestrator æª¢æŸ¥çš„æ˜¯ `labor_law.get("citations")`ï¼š
```python
if labor_law.get("citations"):  # â† æ°¸é æ˜¯ None/ç©ºï¼
    for citation in labor_law["citations"]:
        context["sources"].append({"type": "law", ...})  # â† æ°¸é ä¸åŸ·è¡Œ
```

**çµæœ**: ç´”æ³•è¦å•é¡Œçš„ sources æ°¸é ç‚ºç©º â†’ æœ€é«˜åªå¾— 2/3

**ä¿®å¾©æ–¹æ¡ˆ**:  
åœ¨ `chat_orchestrator.py` çš„ `_build_context` æ–¹æ³•ä¸­ï¼Œç•¶ `has_labor_law=True` ä½†ç„¡çµæ§‹åŒ– `citations` æ™‚ï¼Œç”¨æ­£å‰‡è§£æ `answer` ä¸­çš„æ³•æ¢å¼•ç”¨ï¼Œæˆ–è‡³å°‘åŠ ä¸€å€‹é€šç”¨ sourceï¼š

```python
if has_labor_law:
    # ... åŸæœ‰ citations è™•ç† ...
    
    # è‹¥ Core API æ²’æœ‰çµæ§‹åŒ– citationsï¼Œä½†æœ‰æœ‰æ•ˆå›ç­”ï¼ŒåŠ é€šç”¨ä¾†æº
    if not labor_law.get("citations") and labor_law.get("answer"):
        # å˜—è©¦å¾å›ç­”æ–‡å­—ä¸­è§£ææ³•æ¢å¼•ç”¨
        import re
        law_refs = re.findall(r'ã€Š(.+?)ã€‹(?:ç¬¬(\d+[-ä¹‹]?\d*æ¢?))?', labor_law["answer"])
        if law_refs:
            seen = set()
            for law_name, article in law_refs[:5]:  # æœ€å¤šå– 5 å€‹
                key = f"{law_name} {article}".strip()
                if key not in seen:
                    seen.add(key)
                    context["sources"].append({
                        "type": "law",
                        "title": key,
                        "snippet": labor_law["answer"][:200],
                    })
        else:
            # ç„¡æ³•è§£æï¼ŒåŠ é€šç”¨ä¾†æº
            context["sources"].append({
                "type": "law",
                "title": "å‹å‹•æ³•è¦ (Core API)",
                "snippet": labor_law["answer"][:200],
            })
```

---

### â„¹ï¸ å•é¡Œ E: LLM å›ç­”å»¶é²é«˜ (å¹³å‡ 19-28 ç§’)

**ç¾è±¡**: æ¯å€‹å•é¡Œå¹³å‡éœ€è¦ 19-28 ç§’

**æ ¹å› **: æ¶æ§‹æ€§å»¶é²ï¼Œé Bugã€‚å›ç­”ä¸€é¡Œéœ€è¦ï¼š

| æ­¥é©Ÿ | è€—æ™‚ | èªªæ˜ |
|------|------|------|
| 1. æœ¬åœ° KB èªæ„æœå°‹ | ~0.5s | VoyageAI embed query + pgvector è¿‘é„°æœå°‹ |
| 2. Core API (unihr) é ç«¯å‘¼å« | **10-20s** | GPT-4o ç”Ÿæˆ + Pinecone æœå°‹ |
| 3. åˆæˆå›ç­” | ~3-5s | GPT-4o-mini å°‡é›™æºå…§å®¹åˆæˆ |
| **ç¸½è¨ˆ** | **~15-25s** | æ­¥é©Ÿ 1+2 ä¸¦è¡Œï¼Œæ­¥é©Ÿ 3 åºåˆ— |

**æœ€å¤§ç“¶é ¸**: Core API ä½¿ç”¨ GPT-4o (è¼ƒæ…¢ä½†ç²¾æº–)  

**å„ªåŒ–æ–¹æ¡ˆ** (éå¿…è¦):
- Core API æ”¹ç”¨ GPT-4o-mini â†’ å»¶é²é™è‡³ 5-10sï¼ˆéœ€ä¿®æ”¹ unihr Core è¨­å®šï¼‰
- åŠ å¿«å–: Redis å¿«å–å¸¸è¦‹å•ç­” (TTL=300s, å·²æœ‰æ¶æ§‹ä½†ä¾è³´ KB æœ‰è³‡æ–™)
- ä¸²æµå›ç­”: å‰ç«¯ä½¿ç”¨ SSE ç«¯é» `/api/v1/chat/stream` â†’ ä½¿ç”¨è€…æ›´å¿«çœ‹åˆ°é¦–å­—

---

### â„¹ï¸ å•é¡Œ F: å°æª”æ¡ˆ "No valid chunks" 

**ç¾è±¡**: åªå« "This is a test file" çš„ç´”æ–‡å­—ä¸Šå‚³å¾Œ Worker å›å ± "No valid chunks"

**æ ¹å› **: TextChunker çš„ `chunk_size=1000` tokensã€‚å¦‚æœæ•´ç¯‡æ–‡å­—ä¸åˆ°å¹¾åå€‹ tokenï¼Œchunking å¾Œå¾—åˆ°ç©ºåˆ—è¡¨ã€‚

**å½±éŸ¿**: åªå½±éŸ¿æ¥µå°æ¸¬è©¦æª”æ¡ˆï¼Œæ­£å¼æ–‡ä»¶ä¸å—å½±éŸ¿ (å·²é©—è­‰)

**ä¿®å¾©æ–¹æ¡ˆ** (ä½å„ªå…ˆ):
```python
# document_tasks.py ç¬¬ 83-90 è¡Œé™„è¿‘
chunks = TextChunker.split_by_tokens(...)
if not chunks and text_content.strip():
    # æ–‡å­—å¤ªçŸ­ç„¡æ³•åˆ†å‰²ï¼Œæ•´æ®µä½œç‚ºä¸€å€‹ chunk
    chunks = [text_content.strip()]
```

---

## éœ€è¦ä»€éº¼ï¼Ÿç¼ºå°‘ä»€éº¼ï¼Ÿ

### âŒ ä¸ç¼º LLM
- **OpenAI API Key**: `sk-proj-G3C...` âœ… å·²è¨­å®šï¼ŒGPT-4o-mini æ­£å¸¸é‹ä½œ
- **VoyageAI API Key**: `pa-GpSe...` âœ… å·²è¨­å®šï¼Œembedding æ­£å¸¸é‹ä½œ
- **LlamaParse API Key**: `llx-eBnX...` âœ… å·²è¨­å®šï¼Œé«˜å“è³ªæ–‡ä»¶è§£æå¯ç”¨
- **Core API (unihr)**: `https://ai.unihr.com.tw` âœ… æ­£å¸¸å›æ‡‰

### âœ… æ‰€æœ‰ API é‡‘é‘°æ­£ç¢ºè¼‰å…¥
```
Web Container:   VOYAGE âœ…  OPENAI âœ…  LLAMA âœ…  CORE_API âœ…
Worker Container: VOYAGE âœ…  OPENAI âœ…  LLAMA âœ…
```
é€é pydantic-settings è®€å– `/code/.env`ï¼Œéç³»çµ±ç’°å¢ƒè®Šæ•¸ã€‚

### ç¼ºå°‘çš„æ˜¯ã€Œè³‡æ–™ã€è€Œéã€Œå…ƒä»¶ã€
- documentchunks è¡¨ç‚ºç©º â†’ éœ€è¦é‡æ–°ä¸Šå‚³ 20 ä»½æ¸¬è©¦æ–‡ä»¶
- Core API æ²’æœ‰ citations çµæ§‹ â†’ éœ€è¦ç¨‹å¼ç¢¼é©é…

---

## ä¿®å¾©è¡Œå‹•æ¸…å–®

| å„ªå…ˆç´š | è¡Œå‹• | é è¨ˆå·¥æ™‚ | æ•ˆæœ |
|--------|------|---------|------|
| P0 | é‡æ–°ä¸Šå‚³ 20 ä»½æ¸¬è©¦æ–‡ä»¶ | 5 min | å…¬å¸å…§è¦å•ç­”æ¢å¾© |
| P1 | ä¿® `chat_orchestrator.py` è§£æ Core API æ³•æ¢å¼•ç”¨ | 10 min | æ³•è¦ sources éç©º â†’ è©•åˆ† 3/3 |
| P2 | ä¿® `document_tasks.py` å°æª”æ¡ˆ fallback | 5 min | æ¥µå°æ–‡ä»¶ä¹Ÿèƒ½è™•ç† |
| P3 | å‰ç«¯æ”¹ç”¨ SSE ä¸²æµç«¯é» | å·²æœ‰ç¨‹å¼ç¢¼ | æ¸›å°‘ä½¿ç”¨è€…ç­‰å¾…æ„Ÿ |

---

## é æœŸä¿®å¾©å¾Œæ¸¬è©¦çµæœ

| é …ç›® | ä¿®å¾©å‰ | ä¿®å¾©å¾Œé æ¸¬ |
|------|--------|-----------|
| æ–‡ä»¶ä¸Šå‚³ | 11/11 âœ… (ä½† Worker æœªè™•ç†) | 11/11 âœ… + Worker å®Œæˆè™•ç† |
| å…¬å¸å…§è¦å•ç­” | 0/N å¼•ç”¨å…¬å¸æ”¿ç­– | å…¨éƒ¨å¼•ç”¨å…¬å¸æ”¿ç­– |
| æ³•è¦å•ç­” | å›ç­”æ­£ç¢ºä½†ç„¡ sources | å›ç­”æ­£ç¢º + çµæ§‹åŒ–æ³•æ¢ sources |
| è©•åˆ† | 86/129 (66.7%) | é ä¼° 115-125/129 (89-97%) |
| å»¶é² | 19-28s | 15-20s (ç„¡æ¶æ§‹è®Šæ›´) |

---

## çµè«–

ç³»çµ±**æ¶æ§‹å¥å…¨**ï¼Œæ‰€æœ‰å…ƒä»¶ï¼ˆFastAPI, Celery, pgvector, VoyageAI, OpenAI, LlamaParse, Core APIï¼‰éƒ½æ­£å¸¸é‹ä½œã€‚  
å•é¡Œå…¨éƒ¨æºè‡ª**éƒ¨ç½²è¨­å®šä¸å®Œæ•´**ï¼ˆDB é·ç§»æœªè·‘å®Œã€Celery å•Ÿå‹•è¨­å®šéŒ¯èª¤ï¼‰å’Œ**ä¸€å€‹å°å‹ä»‹é¢ä¸åŒ¹é…**ï¼ˆCore API citations æ ¼å¼ï¼‰ã€‚

**ä¸ç¼ºå°‘ä»»ä½• LLM æˆ– AI å…ƒä»¶ã€‚æ‰€æœ‰ API Key éƒ½å·²æ­£ç¢ºè¨­å®šã€‚**
